const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const compression = require('compression');
const morgan = require('morgan');
const bodyParser = require('body-parser');
const { RateLimiterMemory } = require('rate-limiter-flexible');
const logger = require('./utils/logger');
const AIModel = require('./models/AIModel');
const TextProcessor = require('./utils/textProcessor');
const Cache = require('./utils/cache');

const app = express();
const PORT = process.env.PORT || 3000;

// Middlewares de seguranÃ§a e otimizaÃ§Ã£o
app.use(helmet());
app.use(compression());
app.use(cors({
  origin: '*',
  methods: ['GET', 'POST', 'PUT', 'DELETE'],
  allowedHeaders: ['Content-Type', 'Authorization']
}));
app.use(bodyParser.json({ limit: '10mb' }));
app.use(bodyParser.urlencoded({ extended: true, limit: '10mb' }));
app.use(morgan('combined', { stream: { write: msg => logger.info(msg.trim()) } }));

// Rate Limiting
const rateLimiter = new RateLimiterMemory({
  points: 100,
  duration: 60
});

const rateLimiterMiddleware = async (req, res, next) => {
  try {
    await rateLimiter.consume(req.ip);
    next();
  } catch {
    res.status(429).json({ error: 'Muitas requisiÃ§Ãµes. Tente novamente em breve.' });
  }
};

// InicializaÃ§Ã£o
const aiModel = new AIModel();
const textProcessor = new TextProcessor();
const cache = new Cache();

let modelLoaded = false;

// Carregar modelo ao iniciar
(async () => {
  try {
    logger.info('Carregando modelo de IA...');
    await aiModel.loadModel();
    modelLoaded = true;
    logger.info('Modelo carregado com sucesso!');
  } catch (error) {
    logger.error('Erro ao carregar modelo:', error);
    modelLoaded = false;
  }
})();

// Middleware de verificaÃ§Ã£o do modelo
const checkModel = (req, res, next) => {
  if (!modelLoaded) {
    return res.status(503).json({ 
      error: 'Modelo ainda nÃ£o carregado. Tente novamente em alguns segundos.' 
    });
  }
  next();
};

// Rotas

// Health check
app.get('/health', (req, res) => {
  res.json({
    status: 'online',
    modelLoaded,
    timestamp: new Date().toISOString(),
    uptime: process.uptime()
  });
});

// InformaÃ§Ãµes do modelo
app.get('/api/info', checkModel, (req, res) => {
  res.json({
    name: 'AI Text Generation System',
    version: '1.0.0',
    model: aiModel.getInfo(),
    capabilities: [
      'GeraÃ§Ã£o de texto',
      'ContinuaÃ§Ã£o de texto',
      'Resposta a perguntas',
      'Resumo de texto',
      'AnÃ¡lise de sentimento'
    ]
  });
});

// GeraÃ§Ã£o de texto
app.post('/api/generate', rateLimiterMiddleware, checkModel, async (req, res) => {
  try {
    const { prompt, maxLength = 100, temperature = 0.8, topK = 40 } = req.body;

    if (!prompt || typeof prompt !== 'string') {
      return res.status(400).json({ error: 'Prompt invÃ¡lido' });
    }

    // Verificar cache
    const cacheKey = `gen:${prompt}:${maxLength}:${temperature}`;
    const cached = cache.get(cacheKey);
    if (cached) {
      return res.json({ text: cached, cached: true });
    }

    logger.info(`Gerando texto para prompt: "${prompt.substring(0, 50)}..."`);

    // Processar prompt
    const processedPrompt = textProcessor.preprocess(prompt);

    // Gerar texto
    const generatedText = await aiModel.generateText(processedPrompt, {
      maxLength,
      temperature,
      topK
    });

    // PÃ³s-processar
    const finalText = textProcessor.postprocess(generatedText);

    // Salvar no cache
    cache.set(cacheKey, finalText);

    res.json({
      text: finalText,
      metadata: {
        promptLength: prompt.length,
        generatedLength: finalText.length,
        temperature,
        maxLength
      }
    });

  } catch (error) {
    logger.error('Erro na geraÃ§Ã£o:', error);
    res.status(500).json({ error: 'Erro ao gerar texto', details: error.message });
  }
});

// AnÃ¡lise de sentimento
app.post('/api/analyze', rateLimiterMiddleware, checkModel, async (req, res) => {
  try {
    const { text } = req.body;

    if (!text || typeof text !== 'string') {
      return res.status(400).json({ error: 'Texto invÃ¡lido' });
    }

    const sentiment = await textProcessor.analyzeSentiment(text);
    const entities = textProcessor.extractEntities(text);
    const keywords = textProcessor.extractKeywords(text);

    res.json({
      sentiment,
      entities,
      keywords,
      stats: {
        length: text.length,
        words: text.split(/\s+/).length,
        sentences: text.split(/[.!?]+/).length
      }
    });

  } catch (error) {
    logger.error('Erro na anÃ¡lise:', error);
    res.status(500).json({ error: 'Erro ao analisar texto' });
  }
});

// Resumo de texto
app.post('/api/summarize', rateLimiterMiddleware, checkModel, async (req, res) => {
  try {
    const { text, sentences = 3 } = req.body;

    if (!text || typeof text !== 'string') {
      return res.status(400).json({ error: 'Texto invÃ¡lido' });
    }

    const summary = textProcessor.summarize(text, sentences);

    res.json({
      summary,
      original_length: text.length,
      summary_length: summary.length,
      compression_ratio: (summary.length / text.length * 100).toFixed(2) + '%'
    });

  } catch (error) {
    logger.error('Erro no resumo:', error);
    res.status(500).json({ error: 'Erro ao resumir texto' });
  }
});

// Treinamento (endpoint protegido)
app.post('/api/train', rateLimiterMiddleware, async (req, res) => {
  try {
    const { texts, epochs = 10, batchSize = 32 } = req.body;

    if (!texts || !Array.isArray(texts) || texts.length === 0) {
      return res.status(400).json({ error: 'Dados de treinamento invÃ¡lidos' });
    }

    logger.info(`Iniciando treinamento com ${texts.length} textos`);

    const result = await aiModel.train(texts, { epochs, batchSize });

    res.json({
      message: 'Treinamento concluÃ­do',
      result,
      trained_samples: texts.length
    });

  } catch (error) {
    logger.error('Erro no treinamento:', error);
    res.status(500).json({ error: 'Erro ao treinar modelo' });
  }
});

// Completar texto
app.post('/api/complete', rateLimiterMiddleware, checkModel, async (req, res) => {
  try {
    const { text, numSuggestions = 3 } = req.body;

    if (!text || typeof text !== 'string') {
      return res.status(400).json({ error: 'Texto invÃ¡lido' });
    }

    const suggestions = await aiModel.complete(text, numSuggestions);

    res.json({
      suggestions,
      original: text
    });

  } catch (error) {
    logger.error('Erro na completaÃ§Ã£o:', error);
    res.status(500).json({ error: 'Erro ao completar texto' });
  }
});

// Limpar cache
app.post('/api/cache/clear', (req, res) => {
  cache.clear();
  res.json({ message: 'Cache limpo com sucesso' });
});

// EstatÃ­sticas do cache
app.get('/api/cache/stats', (req, res) => {
  res.json(cache.stats());
});

// Error handler
app.use((err, req, res, next) => {
  logger.error('Erro nÃ£o tratado:', err);
  res.status(500).json({ 
    error: 'Erro interno do servidor',
    message: process.env.NODE_ENV === 'development' ? err.message : undefined
  });
});

// 404 handler
app.use((req, res) => {
  res.status(404).json({ error: 'Rota nÃ£o encontrada' });
});

// Iniciar servidor
app.listen(PORT, () => {
  logger.info(`ðŸš€ Servidor rodando na porta ${PORT}`);
  logger.info(`ðŸ“Š Ambiente: ${process.env.NODE_ENV || 'development'}`);
  logger.info(`ðŸ¤– Modelo: ${modelLoaded ? 'Carregado' : 'Carregando...'}`);
});

// Graceful shutdown
process.on('SIGTERM', () => {
  logger.info('SIGTERM recebido. Encerrando graciosamente...');
  process.exit(0);
});

process.on('SIGINT', () => {
  logger.info('SIGINT recebido. Encerrando graciosamente...');
  process.exit(0);
});